{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SplitNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3iu4VMf9WHi",
        "colab_type": "code",
        "outputId": "4055f5c3-846c-47ad-f861-98fb2615eced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "! pip install syft "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft in /usr/local/lib/python3.6/dist-packages (0.1.25a1)\n",
            "Requirement already satisfied: flask-socketio>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from syft) (4.2.1)\n",
            "Requirement already satisfied: tf-encrypted!=0.5.7,>=0.5.4 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.8)\n",
            "Requirement already satisfied: websocket-client>=0.56.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.56.0)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.3)\n",
            "Requirement already satisfied: lz4>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from syft) (2.1.10)\n",
            "Requirement already satisfied: torch==1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: torchvision==0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.6/dist-packages (from syft) (8.0.2)\n",
            "Requirement already satisfied: zstd>=1.4.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.3.2)\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: msgpack>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Requirement already satisfied: python-socketio>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from flask-socketio>=3.3.2->syft) (4.3.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted!=0.5.7,>=0.5.4->syft) (5.1.2)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.56.0->syft) (1.12.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: python-engineio>=3.9.0 in /usr/local/lib/python3.6/dist-packages (from python-socketio>=4.3.0->flask-socketio>=3.3.2->syft) (3.9.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.33.6)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (41.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56sE61oiDvBT",
        "colab_type": "code",
        "outputId": "dee89a7d-d97d-4a7a-8f2e-973efde722a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "! pip install tf_encrypted"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf_encrypted in ./PySyft/tf-encrypted (0.5.5)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf_encrypted) (1.14.0rc1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tf_encrypted) (1.16.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf_encrypted) (5.1.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (1.14.0rc1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (0.1.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (0.33.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (1.11.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (1.13.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf_encrypted) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<2,>=1.12.0->tf_encrypted) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<2,>=1.12.0->tf_encrypted) (0.15.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf_encrypted) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqZl-5V2yjRm",
        "colab_type": "text"
      },
      "source": [
        "###  Federated Learning CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kOjSp3E9PJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "#torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "#torch.set_default_tensor_type(torch.cuda.FloatTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjVGMR0_9fdD",
        "colab_type": "code",
        "outputId": "e4c10b57-fecc-4cdb-eebb-f6d3b07374fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "import syft as sy  # <-- NEW: import the Pysyft library\n",
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0903 09:28:43.631935 140105902241664 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0903 09:28:43.650582 140105902241664 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIO20I-6QHWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLIENTS = 3383\n",
        "SAMPLES = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrM5OgEAQZgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clients=[]\n",
        "for i in range(NUM_CLIENTS):\n",
        "        clients.append(sy.VirtualWorker(hook, id=\"c \"+str(i)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO5F5uPy9iAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 20\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = True\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc9jOQAeQzIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b73df283-8b75-4dd6-a320-4e39e85ec336"
      },
      "source": [
        "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 4292693.34it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 68774.94it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 1129398.23it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 25752.00it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hHmPDyGUOG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03bc8568-a6a9-4c34-9ec2-3b97f8430c84"
      },
      "source": [
        "A = range(11)\n",
        "A[2:8]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(2, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9FH9OWJVBAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "9f816031-a5c0-40f6-9267-8fc210d35967"
      },
      "source": [
        "train_dataset[0].targets"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-04a3bcef7269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'targets'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovXNQdgTiqSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_idx = int(len(train_dataset)/3383)\n",
        "print(train_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlTitrhoSYBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = list(train_dataset)[:train_idx*SAMPLES]\n",
        "train,labels=[],[]\n",
        "for i in range(SAMPLES):\n",
        "  L,K = [],[]\n",
        "  for j in range(train_idx):\n",
        "    L.append(A[SAMPLES*i+j][0])\n",
        "    K.append(A[SAMPLES*i+j][1])\n",
        "  train.append(L)\n",
        "  labels.append(K)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkMddl7cmTW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "dfff63e2-6f24-45e1-979e-54cc4f60dbb8"
      },
      "source": [
        "train = torch.tensor(train)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-6feccd46a670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36mnew_tensor\u001b[0;34m(owner, id, register, *args, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mnew_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0mcurrent_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnative_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m             \u001b[0m_apply_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: not a sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzcPSLknQvo-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "d6c78600-4f49-4f5d-c57f-097fae3f05e9"
      },
      "source": [
        "# Sending datasets to virtual workers\n",
        "fed_train_dataset = []\n",
        "for i in range(SAMPLES):\n",
        "  print(i)\n",
        "  fed_train_dataset.append(sy.BaseDataset(train[i],labels[i]).send(\"c \"+str(i)))\n",
        "  \n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-c0ff6d86d0f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mfed_train_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"c \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/federated/dataset.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, worker)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \"\"\"\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'send_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c64Vz0a6itE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ74xYtQ9lMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdR1g1XD1vna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "1d99d372-01ce-4c38-cdec-e3fe03612016"
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate(clients), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8802195.80it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 134108.24it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2203807.57it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 50919.69it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqf55r_-upE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f8cbc57b-06cf-4124-df29-f12b5e267a3e"
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data',split='digits', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate(clients), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-96e868ab79d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                    transform=transforms.Compose([\n\u001b[1;32m      4\u001b[0m                        \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                        \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1307\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.3081\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                    ]))\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfederate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA-TMwD8zBp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "980a3b5b-c54a-4066-b271-7fba6c7dda5c"
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.EMNIST('../data',split='digits' ,train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4f79de978f49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     datasets.EMNIST('../data',split='digits' ,train=False, transform=transforms.Compose([\n\u001b[1;32m      3\u001b[0m                        \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                        \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1307\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.3081\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                    ])),\n\u001b[1;32m      6\u001b[0m     batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMNIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             raise RuntimeError('Dataset not found.' +\n\u001b[0;32m---> 72\u001b[0;31m                                ' You can use download=True to download it')\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found. You can use download=True to download it"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEXOJQTT9pod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ6fU0TctNfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc = nn.Linear(784,10)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = x.view(-1,784)\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x,dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdlhPmyC9tZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    i = 0\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        if i < 11:\n",
        "          i+=1\n",
        "          model.send(data.location) # <-- NEW: send the model to the right location\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = model(data)\n",
        "          loss = F.nll_loss(output, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          model.get() # <-- NEW: get the model back\n",
        "          #if batch_idx % args.log_interval == 0:\n",
        "          loss = loss.get() # <-- NEW: get the loss back\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "                  100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "        else:\n",
        "          break\n",
        "        \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dE28E1k9wg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H0mpHVVcHf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_batches(worker, batches, model_in, device, lr,epoch,clients_mem):\n",
        "    \"\"\"Train the model on the worker on the provided batches\n",
        "    Args:\n",
        "        worker(syft.workers.BaseWorker): worker on which the\n",
        "        training will be executed\n",
        "        batches: batches of data of this worker\n",
        "        model_in: machine learning model, training will be done on a copy\n",
        "        device (torch.device): where to run the training\n",
        "        lr: learning rate of the training steps\n",
        "    Returns:\n",
        "        model, loss: obtained model and loss after training\n",
        "    \"\"\"\n",
        "    model = model_in.copy()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)  # TODO momentum is not supported at the moment\n",
        "\n",
        "    model.train()\n",
        "    model.send(worker)\n",
        "    loss_local = False\n",
        "    D = {\"alice\":0,\"bob\":1,\"charlie\":2}\n",
        "    i = D[worker.id]\n",
        "    for batch_idx, (data, target) in enumerate(batches):\n",
        "        loss_local = False\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = f.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            loss = loss.get()  # <-- NEW: get the loss back\n",
        "            loss_local = True\n",
        "            print(\n",
        "                \"Epoch {} Train Worker {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    worker.id,\n",
        "                    batch_idx,\n",
        "                    len(batches),\n",
        "                    100.0 * batch_idx / len(batches),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "    if not loss_local:\n",
        "        loss = loss.get()  # <-- NEW: get the loss back\n",
        "    model.get()  # <-- NEW: get the model back\n",
        "    clients_mem[i] += 2*model_size(model)\n",
        "    return model, loss\n",
        "\n",
        "\n",
        "def get_next_batches(fdataloader: sy.FederatedDataLoader, nr_batches: int):\n",
        "    \"\"\"retrieve next nr_batches of the federated data loader and group\n",
        "    the batches by worker\n",
        "    Args:\n",
        "        fdataloader (sy.FederatedDataLoader): federated data loader\n",
        "        over which the function will iterate\n",
        "        nr_batches (int): number of batches (per worker) to retrieve\n",
        "    Returns:\n",
        "        Dict[syft.workers.BaseWorker, List[batches]]\n",
        "    \"\"\"\n",
        "    batches = {}\n",
        "    for worker_id in fdataloader.workers:\n",
        "        worker = fdataloader.federated_dataset.datasets[worker_id].location\n",
        "        batches[worker] = []\n",
        "    try:\n",
        "        for i in range(nr_batches):\n",
        "            next_batches = next(fdataloader)\n",
        "            for worker in next_batches:\n",
        "                batches[worker].append(next_batches[worker])\n",
        "    except StopIteration:\n",
        "        pass\n",
        "    return batches\n",
        "\n",
        "\n",
        "def train(model, device, federated_train_loader, lr, federate_after_n_batches,epoch,clients_mem):\n",
        "    model.train()\n",
        "\n",
        "    nr_batches = federate_after_n_batches\n",
        "\n",
        "    models = {}\n",
        "    loss_values = {}\n",
        "\n",
        "    iter(federated_train_loader)  # initialize iterators\n",
        "    batches = get_next_batches(federated_train_loader, nr_batches)\n",
        "    counter = 0\n",
        "\n",
        "    while True:\n",
        "        print(\n",
        "            \"Starting training round, batches [{}, {}]\".format(counter, counter + nr_batches)\n",
        "        )\n",
        "        data_for_all_workers = True\n",
        "        for worker in batches:\n",
        "            curr_batches = batches[worker]\n",
        "            if curr_batches:\n",
        "                models[worker], loss_values[worker] = train_on_batches(\n",
        "                    worker, curr_batches, model, device, lr,epoch,clients_mem\n",
        "                )\n",
        "            else:\n",
        "                data_for_all_workers = False\n",
        "        counter += nr_batches\n",
        "        if not data_for_all_workers:\n",
        "            print(\"At least one worker ran out of data, stopping.\")\n",
        "            break\n",
        "\n",
        "        model = utils.federated_avg(models)\n",
        "        batches = get_next_batches(federated_train_loader, nr_batches)\n",
        "    return model\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += f.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
        "    print(\n",
        "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss, correct, len(test_loader.dataset), accuracy\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfSxG6YV9yu6",
        "colab_type": "code",
        "outputId": "116352de-5b35-47c9-8f5d-f5fd47d92a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "#%%time\n",
        "start = time.time()\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "    \n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.533023\n",
            "Train Epoch: 1 [20/60000 (0%)]\tLoss: 2.231701\n",
            "Train Epoch: 1 [40/60000 (0%)]\tLoss: 2.097592\n",
            "Train Epoch: 1 [60/60000 (0%)]\tLoss: 1.941655\n",
            "Train Epoch: 1 [80/60000 (0%)]\tLoss: 1.561559\n",
            "Train Epoch: 1 [100/60000 (0%)]\tLoss: 1.522289\n",
            "Train Epoch: 1 [120/60000 (0%)]\tLoss: 1.483977\n",
            "Train Epoch: 1 [140/60000 (0%)]\tLoss: 1.816720\n",
            "Train Epoch: 1 [160/60000 (0%)]\tLoss: 1.581472\n",
            "Train Epoch: 1 [180/60000 (0%)]\tLoss: 1.513144\n",
            "Train Epoch: 1 [200/60000 (0%)]\tLoss: 1.491247\n",
            "\n",
            "Test set: Average loss: 1.4848, Accuracy: 5787/10000 (58%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.291868\n",
            "Train Epoch: 2 [20/60000 (0%)]\tLoss: 1.304456\n",
            "Train Epoch: 2 [40/60000 (0%)]\tLoss: 1.289812\n",
            "Train Epoch: 2 [60/60000 (0%)]\tLoss: 1.031413\n",
            "Train Epoch: 2 [80/60000 (0%)]\tLoss: 0.841145\n",
            "Train Epoch: 2 [100/60000 (0%)]\tLoss: 0.802011\n",
            "Train Epoch: 2 [120/60000 (0%)]\tLoss: 0.782497\n",
            "Train Epoch: 2 [140/60000 (0%)]\tLoss: 1.243627\n",
            "Train Epoch: 2 [160/60000 (0%)]\tLoss: 1.060057\n",
            "Train Epoch: 2 [180/60000 (0%)]\tLoss: 0.947330\n",
            "Train Epoch: 2 [200/60000 (0%)]\tLoss: 1.013326\n",
            "\n",
            "Test set: Average loss: 1.1971, Accuracy: 6695/10000 (67%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.829169\n",
            "Train Epoch: 3 [20/60000 (0%)]\tLoss: 0.983668\n",
            "Train Epoch: 3 [40/60000 (0%)]\tLoss: 0.948746\n",
            "Train Epoch: 3 [60/60000 (0%)]\tLoss: 0.663396\n",
            "Train Epoch: 3 [80/60000 (0%)]\tLoss: 0.586489\n",
            "Train Epoch: 3 [100/60000 (0%)]\tLoss: 0.526568\n",
            "Train Epoch: 3 [120/60000 (0%)]\tLoss: 0.540872\n",
            "Train Epoch: 3 [140/60000 (0%)]\tLoss: 0.977406\n",
            "Train Epoch: 3 [160/60000 (0%)]\tLoss: 0.809679\n",
            "Train Epoch: 3 [180/60000 (0%)]\tLoss: 0.688389\n",
            "Train Epoch: 3 [200/60000 (0%)]\tLoss: 0.782290\n",
            "\n",
            "Test set: Average loss: 1.0608, Accuracy: 6989/10000 (70%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.613099\n",
            "Train Epoch: 4 [20/60000 (0%)]\tLoss: 0.810564\n",
            "Train Epoch: 4 [40/60000 (0%)]\tLoss: 0.757501\n",
            "Train Epoch: 4 [60/60000 (0%)]\tLoss: 0.480857\n",
            "Train Epoch: 4 [80/60000 (0%)]\tLoss: 0.463167\n",
            "Train Epoch: 4 [100/60000 (0%)]\tLoss: 0.393572\n",
            "Train Epoch: 4 [120/60000 (0%)]\tLoss: 0.422817\n",
            "Train Epoch: 4 [140/60000 (0%)]\tLoss: 0.820557\n",
            "Train Epoch: 4 [160/60000 (0%)]\tLoss: 0.659398\n",
            "Train Epoch: 4 [180/60000 (0%)]\tLoss: 0.543319\n",
            "Train Epoch: 4 [200/60000 (0%)]\tLoss: 0.645709\n",
            "\n",
            "Test set: Average loss: 0.9824, Accuracy: 7136/10000 (71%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.488496\n",
            "Train Epoch: 5 [20/60000 (0%)]\tLoss: 0.696064\n",
            "Train Epoch: 5 [40/60000 (0%)]\tLoss: 0.632389\n",
            "Train Epoch: 5 [60/60000 (0%)]\tLoss: 0.375926\n",
            "Train Epoch: 5 [80/60000 (0%)]\tLoss: 0.387825\n",
            "Train Epoch: 5 [100/60000 (0%)]\tLoss: 0.315402\n",
            "Train Epoch: 5 [120/60000 (0%)]\tLoss: 0.351442\n",
            "Train Epoch: 5 [140/60000 (0%)]\tLoss: 0.710137\n",
            "Train Epoch: 5 [160/60000 (0%)]\tLoss: 0.557569\n",
            "Train Epoch: 5 [180/60000 (0%)]\tLoss: 0.450756\n",
            "Train Epoch: 5 [200/60000 (0%)]\tLoss: 0.551769\n",
            "\n",
            "Test set: Average loss: 0.9314, Accuracy: 7235/10000 (72%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.406189\n",
            "Train Epoch: 6 [20/60000 (0%)]\tLoss: 0.611537\n",
            "Train Epoch: 6 [40/60000 (0%)]\tLoss: 0.542571\n",
            "Train Epoch: 6 [60/60000 (0%)]\tLoss: 0.308948\n",
            "Train Epoch: 6 [80/60000 (0%)]\tLoss: 0.335008\n",
            "Train Epoch: 6 [100/60000 (0%)]\tLoss: 0.263637\n",
            "Train Epoch: 6 [120/60000 (0%)]\tLoss: 0.302815\n",
            "Train Epoch: 6 [140/60000 (0%)]\tLoss: 0.624433\n",
            "Train Epoch: 6 [160/60000 (0%)]\tLoss: 0.482877\n",
            "Train Epoch: 6 [180/60000 (0%)]\tLoss: 0.385713\n",
            "Train Epoch: 6 [200/60000 (0%)]\tLoss: 0.481042\n",
            "\n",
            "Test set: Average loss: 0.8958, Accuracy: 7308/10000 (73%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.346961\n",
            "Train Epoch: 7 [20/60000 (0%)]\tLoss: 0.545183\n",
            "Train Epoch: 7 [40/60000 (0%)]\tLoss: 0.474434\n",
            "Train Epoch: 7 [60/60000 (0%)]\tLoss: 0.262848\n",
            "Train Epoch: 7 [80/60000 (0%)]\tLoss: 0.294821\n",
            "Train Epoch: 7 [100/60000 (0%)]\tLoss: 0.226696\n",
            "Train Epoch: 7 [120/60000 (0%)]\tLoss: 0.267003\n",
            "Train Epoch: 7 [140/60000 (0%)]\tLoss: 0.554625\n",
            "Train Epoch: 7 [160/60000 (0%)]\tLoss: 0.425213\n",
            "Train Epoch: 7 [180/60000 (0%)]\tLoss: 0.336837\n",
            "Train Epoch: 7 [200/60000 (0%)]\tLoss: 0.425095\n",
            "\n",
            "Test set: Average loss: 0.8698, Accuracy: 7344/10000 (73%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.301949\n",
            "Train Epoch: 8 [20/60000 (0%)]\tLoss: 0.491084\n",
            "Train Epoch: 8 [40/60000 (0%)]\tLoss: 0.420813\n",
            "Train Epoch: 8 [60/60000 (0%)]\tLoss: 0.229257\n",
            "Train Epoch: 8 [80/60000 (0%)]\tLoss: 0.262668\n",
            "Train Epoch: 8 [100/60000 (0%)]\tLoss: 0.198950\n",
            "Train Epoch: 8 [120/60000 (0%)]\tLoss: 0.239130\n",
            "Train Epoch: 8 [140/60000 (0%)]\tLoss: 0.496498\n",
            "Train Epoch: 8 [160/60000 (0%)]\tLoss: 0.379117\n",
            "Train Epoch: 8 [180/60000 (0%)]\tLoss: 0.298456\n",
            "Train Epoch: 8 [200/60000 (0%)]\tLoss: 0.379599\n",
            "\n",
            "Test set: Average loss: 0.8502, Accuracy: 7374/10000 (74%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.266508\n",
            "Train Epoch: 9 [20/60000 (0%)]\tLoss: 0.445847\n",
            "Train Epoch: 9 [40/60000 (0%)]\tLoss: 0.377439\n",
            "Train Epoch: 9 [60/60000 (0%)]\tLoss: 0.203667\n",
            "Train Epoch: 9 [80/60000 (0%)]\tLoss: 0.236122\n",
            "Train Epoch: 9 [100/60000 (0%)]\tLoss: 0.177320\n",
            "Train Epoch: 9 [120/60000 (0%)]\tLoss: 0.216557\n",
            "Train Epoch: 9 [140/60000 (0%)]\tLoss: 0.447588\n",
            "Train Epoch: 9 [160/60000 (0%)]\tLoss: 0.341322\n",
            "Train Epoch: 9 [180/60000 (0%)]\tLoss: 0.267433\n",
            "Train Epoch: 9 [200/60000 (0%)]\tLoss: 0.341945\n",
            "\n",
            "Test set: Average loss: 0.8352, Accuracy: 7413/10000 (74%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.237918\n",
            "Train Epoch: 10 [20/60000 (0%)]\tLoss: 0.407352\n",
            "Train Epoch: 10 [40/60000 (0%)]\tLoss: 0.341586\n",
            "Train Epoch: 10 [60/60000 (0%)]\tLoss: 0.183469\n",
            "Train Epoch: 10 [80/60000 (0%)]\tLoss: 0.213761\n",
            "Train Epoch: 10 [100/60000 (0%)]\tLoss: 0.159971\n",
            "Train Epoch: 10 [120/60000 (0%)]\tLoss: 0.197760\n",
            "Train Epoch: 10 [140/60000 (0%)]\tLoss: 0.406152\n",
            "Train Epoch: 10 [160/60000 (0%)]\tLoss: 0.309728\n",
            "Train Epoch: 10 [180/60000 (0%)]\tLoss: 0.241857\n",
            "Train Epoch: 10 [200/60000 (0%)]\tLoss: 0.310355\n",
            "\n",
            "Test set: Average loss: 0.8234, Accuracy: 7438/10000 (74%)\n",
            "\n",
            "16.807437419891357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wCl4Khdyugu",
        "colab_type": "text"
      },
      "source": [
        "### SPLIT LEARNING CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GZ6lP80ddBiV",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = True\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a9ea64bc-e14b-4861-cfbe-96c50c7bb241",
        "id": "0P-qt7WRdBiZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 27476627.61it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 445880.03it/s]\n",
            "  1%|          | 16384/1648877 [00:00<00:11, 142108.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 7442404.28it/s]                            \n",
            "8192it [00:00, 182759.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t0Ly9q7NdBif",
        "colab": {}
      },
      "source": [
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        return F.max_pool2d(x, 2, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTRCegEAfiBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc_jVaQgf9qE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [Net1(),Net1()]\n",
        "models[0] = models[0].send(bob)\n",
        "models[1] = models[1].send(alice)\n",
        "\n",
        "\n",
        "opt1 = optim.SGD(params=models[0].parameters(),lr=0.1)\n",
        "opt2 = optim.SGD(params=models[1].parameters(),lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "feBHg_uKdBig",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targs) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        #IF ON DATA LOCATION TO GET THE RIGHT MODEL\n",
        "        if data.location.id == 'bob':\n",
        "          mod_c,opt_c = models[0], opt1\n",
        "        else : \n",
        "          mod_c,opt_c = models[1], opt2\n",
        "          \n",
        "       # 1) erase previous gradients (if they exist)\n",
        "        optimizer.step()\n",
        "        opt_c.step()\n",
        "        opt_c.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        #model.send(data.location) # <-- NEW: send the model to the right location\n",
        "        #target= tar.get()\n",
        "        tg_copy = targs.copy()\n",
        "        target = tg_copy.get()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        #optimizer.zero_grad()\n",
        "        # 2) make a prediction until cut layer (client location)\n",
        "        pred_c = mod_c(data)\n",
        "        copy = pred_c.copy()\n",
        "        # 3) get this to the server \n",
        "        inp = copy.get()\n",
        "\n",
        "        # 4) make prediction with second part of the model (server location)\n",
        "        pred = model(inp)\n",
        "\n",
        "        # 5) calculate how much we missed \n",
        "        #print(pred.size(),target.size())\n",
        "        loss = F.nll_loss(pred, target)\n",
        "        loss.backward()\n",
        "        \n",
        "        gradient = inp.grad\n",
        "        gradient = gradient.send(data.location)\n",
        "        pred_c.backward(gradient)\n",
        "        \n",
        "       #target = target.send(data.location)\n",
        "        #optimizer.step()\n",
        "        #model.get() # <-- NEW: get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            #loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "                100. * batch_idx / len(federated_train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0wCZFKeZdBim",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    M1 = models[0].copy()\n",
        "    M2 = models[1].copy()\n",
        "    M1 = M1.get()\n",
        "    M2 = M2.get()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(M1(data))\n",
        "            #output2 = model(M2(data))\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "42b06659-8029-488b-938a-e35802923b2b",
        "id": "Q-YKdNQcdBir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6097
        }
      },
      "source": [
        "%%time\n",
        "model = Net2().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.302813\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.123377\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.199097\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 0.594879\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.559763\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.425805\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.171876\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.300623\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.436527\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.656055\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.144281\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.374599\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.114359\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.329657\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.041824\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.124439\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.506185\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.249304\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.408869\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.161326\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.349700\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.206512\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.156588\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.134978\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.195048\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.083174\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.221032\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.116621\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.047030\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.310191\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.044661\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.173666\n",
            "\n",
            "Test set: Average loss: 0.1690, Accuracy: 9458/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.154394\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.058190\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.116271\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.078232\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.129064\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.061197\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.123970\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.114348\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.019508\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.164301\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.143948\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.072989\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.277202\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.171462\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.030080\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.098256\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.113432\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.047648\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.154211\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.043815\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.258654\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.041158\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.039325\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.131609\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.022616\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.084771\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.043423\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.075946\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.087941\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.086574\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.020453\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.211243\n",
            "\n",
            "Test set: Average loss: 0.0878, Accuracy: 9714/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.077961\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.180027\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.035791\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.027055\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.214889\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.135212\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.047810\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.056373\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.009762\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.061180\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.012983\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.047652\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.072666\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.105879\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.080178\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.144260\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.047852\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.135668\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.044649\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.056856\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.126776\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.043234\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.026133\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.082981\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.055316\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.032402\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.041994\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.041057\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.053452\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.069630\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.008330\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.065874\n",
            "\n",
            "Test set: Average loss: 0.0741, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.084150\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.009845\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.067092\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.123745\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.152517\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.033920\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.008069\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.010476\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.263523\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.111362\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.157192\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.044398\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.077355\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.017110\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.076668\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.032969\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.031048\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.015468\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.034330\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.019536\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.176808\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.028516\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.133254\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.022252\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.049060\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.090692\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.007430\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.014867\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.051601\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.033078\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.066969\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.014801\n",
            "\n",
            "Test set: Average loss: 0.0564, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.011322\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.031946\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.007722\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.026008\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.086141\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.072561\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.054575\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.016367\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.006135\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.062874\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.047519\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.003736\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.022920\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.013142\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.206059\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.013699\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.062541\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.009046\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.043004\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.085342\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.042564\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.063046\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.032532\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.043928\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.049930\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.026319\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.020677\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.112312\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.021482\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.090333\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.016205\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.098210\n",
            "\n",
            "Test set: Average loss: 0.0481, Accuracy: 9846/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.051244\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.050279\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.024260\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.016640\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.054422\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.011362\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.025011\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.005094\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.069884\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.007981\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.027671\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.004121\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.012888\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.057706\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.002681\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.002691\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.043580\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.026261\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.011887\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.087619\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.055712\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.021382\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.067204\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.002554\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.050846\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.081170\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.008142\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.024042\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.009465\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.005093\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.026812\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.011063\n",
            "\n",
            "Test set: Average loss: 0.0362, Accuracy: 9881/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.016768\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.011933\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.079431\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.112892\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.009087\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.127674\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.023553\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.031791\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.012029\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.064540\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.003912\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.140471\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.023498\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.006770\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.052744\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.005826\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.025085\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.112247\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.032213\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.018295\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.022887\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.023667\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.004694\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.012900\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.003932\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.138927\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.027812\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.017054\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.022357\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.075250\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.046861\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.015033\n",
            "\n",
            "Test set: Average loss: 0.0363, Accuracy: 9887/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.011996\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.014615\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.037330\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.038754\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.007032\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.013558\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.016146\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.040848\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.022240\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.014862\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.022772\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.036032\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.039658\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.009697\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.026153\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.008273\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.011516\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.159696\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.006114\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.039457\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.024408\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.015289\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.076905\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.043468\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.011849\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.030187\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.017013\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.026980\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.045410\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.009336\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.027123\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.015182\n",
            "\n",
            "Test set: Average loss: 0.0359, Accuracy: 9882/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.022021\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.007195\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.080549\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.073458\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.002832\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.043978\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.006181\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.002318\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.001113\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.007076\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.006033\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.011798\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.002596\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.170817\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.098157\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.037682\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.009251\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.190894\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.014854\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.061753\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.001677\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.002813\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.039098\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.002769\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.013849\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.007692\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.042304\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.061579\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.011985\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.057709\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.117618\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.018752\n",
            "\n",
            "Test set: Average loss: 0.0397, Accuracy: 9875/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.074479\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.011703\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.006611\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.001520\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.006622\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.003403\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.038959\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.000929\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.032976\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.057616\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.026093\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.000680\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.160151\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.030811\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.046432\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.020736\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.003128\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.060239\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.024648\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.036248\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.003581\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.008756\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.005385\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.005475\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.007402\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.023184\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.008189\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.009996\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.042437\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.002723\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.008734\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.000840\n",
            "\n",
            "Test set: Average loss: 0.0280, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "CPU times: user 14min 30s, sys: 58.9 s, total: 15min 28s\n",
            "Wall time: 15min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jgcgpnVNy8S_"
      },
      "source": [
        "###  Federated Learning GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru41m4PDGawj",
        "colab_type": "code",
        "outputId": "b975643d-6bc3-4a6a-abfa-3466f4beae08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "! git clone https://github.com/OpenMined/PySyft.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PySyft'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 27077 (delta 9), reused 16 (delta 6), pack-reused 27052\u001b[K\n",
            "Receiving objects: 100% (27077/27077), 31.68 MiB | 19.72 MiB/s, done.\n",
            "Resolving deltas: 100% (17646/17646), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcgopYHMAPLv",
        "colab_type": "code",
        "outputId": "be6d868f-dcb3-47f2-c670-3880cfdaeafd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mPySyft\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4dZ9RX_P0hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip insg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F05eMKZdy8TB",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8aLBtlRby8TG",
        "outputId": "9503de8a-8639-4c36-abde-a89e5452da33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "import syft as sy  # <-- NEW: import the Pysyft library\n",
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ef9550d4a3a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msyft\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msy\u001b[0m  \u001b[0;31m# <-- NEW: import the Pysyft library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTorchHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVirtualWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bob\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <-- NEW: define remote worker bob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0malice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVirtualWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"alice\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <-- NEW: and alice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'syft'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a4INzVfBy8TM",
        "outputId": "9221b46d-0097-4a4f-9199-3804af506255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1055
        }
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "        # TODO Quickhack. Actually need to fix the problem moving the model to CUDA\\n\",\n",
        "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "\n",
        "#kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "kwargs = {'num_workers': 0, 'pin_memory': False} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook_args.py\u001b[0m in \u001b[0;36mhook_function_args\u001b[0;34m(attr, args, kwargs, return_args_type)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# TODO rename registry or use another one than for methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mhook_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_method_args_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0mget_tensor_type_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tensor_type_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'torch.set_default_tensor_type'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a355fd26f195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# TODO Quickhack. Actually need to fix the problem moving the model to CUDA\\n\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mcmd_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{attr.__module__}.{attr.__name__}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;31m# Note that we return also args_type which helps handling case 3 in the docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             new_args, new_kwargs, new_type, args_type = syft.frameworks.torch.hook_args.hook_function_args(\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_args_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             )\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# This handles case 3: it redirects the command to the appropriate class depending\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook_args.py\u001b[0m in \u001b[0;36mhook_function_args\u001b[0;34m(attr, args, kwargs, return_args_type)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Update the function in case of an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         args_hook_function, get_tensor_type_function = build_hook_args_function(\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         )\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# Store the utility functions in registries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook_args.py\u001b[0m in \u001b[0;36mbuild_hook_args_function\u001b[0;34m(args, return_tuple)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Build a function with this rule to efficiently the child type of the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# tensor found in the args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mget_tensor_type_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_get_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0margs_hook_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_type_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook_args.py\u001b[0m in \u001b[0;36mbuild_get_tensor_type\u001b[0;34m(rules, layer)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "05a3f1de-6033-4257-ca21-484cc01e13e2",
        "id": "9f1aO31ey8TP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8422041.44it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 137291.50it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2231027.05it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 52421.36it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tg6JGH_0y8TW",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ObM-n51ty8Ta",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        model.send(data.location) # <-- NEW: send the model to the right location\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get() # <-- NEW: get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "                100. * batch_idx / len(federated_train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZENkkCDcy8Tf",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "69c5441c-ae78-40fe-8c82-cc396b48fcc9",
        "id": "mznX2JGoy8Tn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6080
        }
      },
      "source": [
        "%%time\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.305134\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.156802\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.896600\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.440342\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.867032\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.654484\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.593135\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.455443\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.370512\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.304318\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.313664\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.369745\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.238009\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.187349\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.524126\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.224937\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.174450\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.201018\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.207578\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.311946\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.139146\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.333254\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.187782\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.199155\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.254292\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.420406\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.275386\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.055030\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.164749\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.280688\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.069393\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.108372\n",
            "\n",
            "Test set: Average loss: 0.1535, Accuracy: 9531/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.114989\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.105406\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.178769\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.088889\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.126971\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.138506\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.088920\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.074801\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.075214\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.168961\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.105120\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.155118\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.174462\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.242706\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.260954\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.104239\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.047104\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.165445\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.136210\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.078888\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.230788\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.097349\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.142231\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.048155\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.071012\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.067565\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.132849\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.037216\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.065252\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.147903\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.140761\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.066787\n",
            "\n",
            "Test set: Average loss: 0.0897, Accuracy: 9732/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.092557\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.044248\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.197166\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.067634\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.131942\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.255649\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.254629\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.063718\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.231195\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.185263\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.039348\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.051545\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.078421\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.020104\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.404320\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.096633\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.072825\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.025908\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.100281\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.035961\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.016558\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.093914\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.129816\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.070581\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.091968\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.019917\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.057498\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.037715\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.065442\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.062178\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.040654\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.589421\n",
            "\n",
            "Test set: Average loss: 0.0758, Accuracy: 9749/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.190088\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.071042\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.039485\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.116442\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.047453\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.019636\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.044014\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.154399\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.048225\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.039363\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.117407\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.014788\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.039764\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.189124\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.238765\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.084807\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.015120\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.214230\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.030683\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.018064\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.051132\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.065346\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.137708\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.078998\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.020411\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.201873\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.102658\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.062533\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.081162\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.092253\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.018689\n",
            "\n",
            "Test set: Average loss: 0.0566, Accuracy: 9808/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.070733\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.046867\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.009137\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.046441\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.100067\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.026823\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.200012\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.011893\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.161808\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.070186\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.035938\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.059042\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.023044\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.016356\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.020930\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.029029\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.051451\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.071672\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.136029\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.051755\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.017212\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.012017\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.020658\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.021178\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.022919\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.016391\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.082108\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.034217\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.009294\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.078122\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.038832\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.025281\n",
            "\n",
            "Test set: Average loss: 0.0472, Accuracy: 9844/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.052357\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.100523\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.005480\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.116208\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.014322\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.018405\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.024613\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.062991\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.101039\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.086084\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.068697\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.079672\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.044973\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.006916\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.036750\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.015647\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.014625\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.010802\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.019010\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.057307\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.008112\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.100586\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.049176\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.013757\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.047341\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.084231\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.026033\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.020376\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.058133\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.015081\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.153308\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.007673\n",
            "\n",
            "Test set: Average loss: 0.0471, Accuracy: 9839/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.083116\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.037696\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.022938\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.050279\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.058285\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.043970\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.094661\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.033384\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.025700\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.099129\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.132635\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.078328\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.004796\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.050426\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.036096\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.004079\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.007948\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.073479\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.040254\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.003530\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.046024\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.035701\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.010823\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.011016\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.035453\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.053504\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.090996\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.024423\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.048883\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.006174\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.227442\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.048987\n",
            "\n",
            "Test set: Average loss: 0.0461, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.052888\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.049164\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.076622\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.114958\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.016372\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.015576\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.003963\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.064083\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.012354\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.024595\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.026211\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.003051\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.120731\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.093562\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.003521\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.004116\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.046946\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.008299\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.023118\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.038781\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.014813\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.048104\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.045090\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.019392\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.062242\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.031651\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.015460\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.040102\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.044916\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.011466\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.123928\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.183758\n",
            "\n",
            "Test set: Average loss: 0.0359, Accuracy: 9887/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.005247\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.013709\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.016216\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.023823\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.075705\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.013724\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.086809\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.005296\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.033389\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.048760\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.026363\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.011354\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.009147\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.054680\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.052351\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.181491\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.008114\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.031524\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.034959\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.005776\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.014528\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.002995\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.050593\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.006476\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.019793\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.012943\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.009479\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.071473\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.002955\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.012385\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.008487\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.001242\n",
            "\n",
            "Test set: Average loss: 0.0352, Accuracy: 9879/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.005012\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.012878\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.039924\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.003015\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.122311\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.003975\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.038626\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.006842\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.002885\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.003915\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.066220\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.050074\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.268545\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.007152\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.005392\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.007273\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.004889\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.004045\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.043609\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.027111\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.021162\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.098772\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.016670\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.005863\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.004863\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.103031\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.045374\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.003337\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.128536\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.003583\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.010670\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.012636\n",
            "\n",
            "Test set: Average loss: 0.0325, Accuracy: 9892/10000 (99%)\n",
            "\n",
            "CPU times: user 15min 24s, sys: 56.1 s, total: 16min 20s\n",
            "Wall time: 16min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6rDZBBxVFPA",
        "colab_type": "text"
      },
      "source": [
        "### Split NN config 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ue_XY-MjVH98",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = True\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2aa7f517-af5a-4959-df23-afd4673c2bdc",
        "id": "CecchBmEVH9_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 18943904.97it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 318986.76it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5231109.26it/s]                           \n",
            "8192it [00:00, 132621.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y94LI1PrVH-I",
        "colab": {}
      },
      "source": [
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        return F.max_pool2d(x, 2, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EiTNgK8ZVH-K",
        "colab": {}
      },
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RQn-PEYkVH-M",
        "colab": {}
      },
      "source": [
        "models = [Net1(),Net1()]\n",
        "models[0] = models[0].send(bob)\n",
        "models[1] = models[1].send(alice)\n",
        "\n",
        "\n",
        "opt1 = optim.SGD(params=models[0].parameters(),lr=0.1)\n",
        "opt2 = optim.SGD(params=models[1].parameters(),lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dca4GkaPVH-P",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targs) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        #IF ON DATA LOCATION TO GET THE RIGHT MODEL\n",
        "        if data.location.id == 'bob':\n",
        "          mod_c,opt_c = models[0], opt1\n",
        "        else : \n",
        "          mod_c,opt_c = models[1], opt2\n",
        "          \n",
        "       # 1) erase previous gradients (if they exist)\n",
        "        optimizer.step()\n",
        "        opt_c.step()\n",
        "        opt_c.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        #model.send(data.location) # <-- NEW: send the model to the right location\n",
        "        #target= tar.get()\n",
        "        #tg_copy = targs.copy()\n",
        "        #target = tg_copy.get()\n",
        "        \n",
        "        data, target = data.to(device), targs.to(device)\n",
        "        #optimizer.zero_grad()\n",
        "        # 2) make a prediction until cut layer (client location)\n",
        "        pred_c = mod_c(data)\n",
        "        copy = pred_c.copy()\n",
        "        # 3) get this to the server \n",
        "        inp = copy.get()\n",
        "\n",
        "        # 4) make prediction with second part of the model (server location)\n",
        "        pred = model(inp)\n",
        "        cop = pred.copy()\n",
        "        out = cop.send(data.location)\n",
        "        # 5) calculate how much we missed \n",
        "        #print(pred.size(),target.size())\n",
        "        loss = F.nll_loss(out, target)\n",
        "        loss.backward()\n",
        "        grad = out.grad\n",
        "        #grad = grad.clone().get()\n",
        "        grad1 = grad.copy()\n",
        "        grad2 = grad1.get()\n",
        "        print(\"grad:\",grad2.shape)\n",
        "        print(\"pred\",pred.shape)\n",
        "        pred.backward(grad2)\n",
        "        \n",
        "        \n",
        "        gradient = inp.grad\n",
        "        print('inp',inp.shape)\n",
        "        print('gradient',gradient.shape)\n",
        "        gradient = gradient.send(data.location)\n",
        "        print(\"pred_c\",pred_c.shape)\n",
        "        print('gradient',gradient.shape)\n",
        "        pred_c.backward(gradient)\n",
        "        \n",
        "        \n",
        "       #target = target.send(data.location)\n",
        "        #optimizer.step()\n",
        "        #model.get() # <-- NEW: get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            #loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "                100. * batch_idx / len(federated_train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kwCmjE7JVH-S",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    M1 = models[0].copy()\n",
        "    M2 = models[1].copy()\n",
        "    M1 = M1.get()\n",
        "    M2 = M2.get()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(M1(data))\n",
        "            #output2 = model(M2(data))\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9c9f7663-1b42-452e-cf01-0044a66eb28c",
        "id": "iqZ9QK-AVH-V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2282
        }
      },
      "source": [
        "%%time\n",
        "model = Net2().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grad: torch.Size([64, 10])\n",
            "pred torch.Size([64, 10])\n",
            "inp torch.Size([64, 20, 12, 12])\n",
            "gradient torch.Size([64, 20, 12, 12])\n",
            "pred_c torch.Size([64, 20, 12, 12])\n",
            "gradient torch.Size([64, 20, 12, 12])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-0325af6d78a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model = Net2().to(device)\\noptimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\\n\\nfor epoch in range(1, args.epochs + 1):\\n    train(args, model, device, federated_train_loader, optimizer, epoch)\\n    test(args, model, device, test_loader)\\n\\nif (args.save_model):\\n    torch.save(model.state_dict(), \"mnist_cnn.pt\")'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-4a24d56be501>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, federated_train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pred_c\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gradient'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mpred_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, message, return_ids)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSGTYPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, msg_type, message, location)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVirtualWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWorker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFederatedClient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseWorker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"worker {self} received {sy.codes.code2MSGTYPE[msg_type]} {contents}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Step 1: route message to appropriate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_router\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmsg_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# Step 2: Serialize the message to simple python objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0;31m# TODO Andrew thinks this is gross, please fix. Instead need to properly deserialize strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mroute_method_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# means that there is a wrapper to remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m                         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: invalid gradient at index 0 - got [0] but expected shape compatible with [64, 20, 12, 12]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqdddLx1XMwo",
        "colab_type": "text"
      },
      "source": [
        "## TESTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bb3wdjI2XPGW",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = True\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EF6HNwdzXPGY",
        "colab": {}
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZeyFbQfBXPGc",
        "colab": {}
      },
      "source": [
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        return F.max_pool2d(x, 2, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anq1dh3GXPGe",
        "colab": {}
      },
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qvrlSs8oXPGg",
        "colab": {}
      },
      "source": [
        "models = [Net1(),Net1()]\n",
        "models[0] = models[0].send(bob)\n",
        "models[1] = models[1].send(alice)\n",
        "\n",
        "\n",
        "opt1 = optim.SGD(params=models[0].parameters(),lr=0.1)\n",
        "opt2 = optim.SGD(params=models[1].parameters(),lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sv-zea67XPGi",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targs) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        #IF ON DATA LOCATION TO GET THE RIGHT MODEL\n",
        "        if data.location.id == 'bob':\n",
        "          mod_c,opt_c = models[0], opt1\n",
        "        else : \n",
        "          mod_c,opt_c = models[1], opt2\n",
        "          \n",
        "       # 1) erase previous gradients (if they exist)\n",
        "        optimizer.step()\n",
        "        opt_c.step()\n",
        "        opt_c.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        #model.send(data.location) # <-- NEW: send the model to the right location\n",
        "        #target= tar.get()\n",
        "        tg_copy = targs.copy()\n",
        "        target = tg_copy.get()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        #optimizer.zero_grad()\n",
        "        # 2) make a prediction until cut layer (client location)\n",
        "        pred_c = mod_c(data)\n",
        "        copy = pred_c.copy()\n",
        "        # 3) get this to the server \n",
        "        inp = copy.get()\n",
        "\n",
        "        # 4) make prediction with second part of the model (server location)\n",
        "        pred = model(inp)\n",
        "\n",
        "        # 5) calculate how much we missed \n",
        "        #print(pred.size(),target.size())\n",
        "        loss = F.nll_loss(pred, target)\n",
        "        loss.backward()\n",
        "        \n",
        "        gradient = inp.grad\n",
        "        gradient = gradient.send(data.location)\n",
        "        pred_c.backward(gradient)\n",
        "        \n",
        "       #target = target.send(data.location)\n",
        "        #optimizer.step()\n",
        "        #model.get() # <-- NEW: get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            #loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "                100. * batch_idx / len(federated_train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s7xoRSG_XPGk",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    M1 = models[0].copy()\n",
        "    M2 = models[1].copy()\n",
        "    M1 = M1.get()\n",
        "    M2 = M2.get()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(M1(data))\n",
        "            #output2 = model(M2(data))\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "42b06659-8029-488b-938a-e35802923b2b",
        "id": "TolLhNCiXPGo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6097
        }
      },
      "source": [
        "%%time\n",
        "model = Net2().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.302813\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.123377\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.199097\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 0.594879\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.559763\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.425805\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.171876\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.300623\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.436527\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.656055\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.144281\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.374599\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.114359\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.329657\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.041824\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.124439\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.506185\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.249304\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.408869\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.161326\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.349700\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.206512\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.156588\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.134978\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.195048\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.083174\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.221032\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.116621\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.047030\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.310191\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.044661\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.173666\n",
            "\n",
            "Test set: Average loss: 0.1690, Accuracy: 9458/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.154394\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.058190\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.116271\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.078232\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.129064\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.061197\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.123970\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.114348\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.019508\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.164301\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.143948\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.072989\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.277202\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.171462\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.030080\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.098256\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.113432\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.047648\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.154211\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.043815\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.258654\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.041158\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.039325\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.131609\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.022616\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.084771\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.043423\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.075946\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.087941\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.086574\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.020453\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.211243\n",
            "\n",
            "Test set: Average loss: 0.0878, Accuracy: 9714/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.077961\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.180027\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.035791\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.027055\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.214889\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.135212\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.047810\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.056373\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.009762\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.061180\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.012983\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.047652\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.072666\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.105879\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.080178\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.144260\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.047852\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.135668\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.044649\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.056856\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.126776\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.043234\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.026133\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.082981\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.055316\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.032402\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.041994\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.041057\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.053452\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.069630\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.008330\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.065874\n",
            "\n",
            "Test set: Average loss: 0.0741, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.084150\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.009845\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.067092\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.123745\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.152517\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.033920\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.008069\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.010476\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.263523\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.111362\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.157192\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.044398\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.077355\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.017110\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.076668\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.032969\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.031048\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.015468\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.034330\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.019536\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.176808\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.028516\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.133254\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.022252\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.049060\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.090692\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.007430\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.014867\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.051601\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.033078\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.066969\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.014801\n",
            "\n",
            "Test set: Average loss: 0.0564, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.011322\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.031946\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.007722\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.026008\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.086141\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.072561\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.054575\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.016367\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.006135\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.062874\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.047519\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.003736\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.022920\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.013142\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.206059\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.013699\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.062541\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.009046\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.043004\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.085342\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.042564\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.063046\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.032532\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.043928\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.049930\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.026319\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.020677\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.112312\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.021482\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.090333\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.016205\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.098210\n",
            "\n",
            "Test set: Average loss: 0.0481, Accuracy: 9846/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.051244\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.050279\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.024260\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.016640\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.054422\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.011362\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.025011\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.005094\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.069884\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.007981\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.027671\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.004121\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.012888\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.057706\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.002681\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.002691\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.043580\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.026261\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.011887\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.087619\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.055712\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.021382\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.067204\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.002554\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.050846\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.081170\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.008142\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.024042\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.009465\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.005093\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.026812\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.011063\n",
            "\n",
            "Test set: Average loss: 0.0362, Accuracy: 9881/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.016768\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.011933\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.079431\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.112892\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.009087\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.127674\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.023553\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.031791\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.012029\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.064540\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.003912\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.140471\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.023498\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.006770\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.052744\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.005826\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.025085\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.112247\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.032213\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.018295\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.022887\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.023667\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.004694\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.012900\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.003932\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.138927\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.027812\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.017054\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.022357\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.075250\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.046861\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.015033\n",
            "\n",
            "Test set: Average loss: 0.0363, Accuracy: 9887/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.011996\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.014615\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.037330\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.038754\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.007032\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.013558\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.016146\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.040848\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.022240\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.014862\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.022772\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.036032\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.039658\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.009697\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.026153\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.008273\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.011516\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.159696\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.006114\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.039457\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.024408\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.015289\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.076905\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.043468\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.011849\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.030187\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.017013\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.026980\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.045410\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.009336\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.027123\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.015182\n",
            "\n",
            "Test set: Average loss: 0.0359, Accuracy: 9882/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.022021\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.007195\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.080549\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.073458\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.002832\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.043978\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.006181\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.002318\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.001113\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.007076\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.006033\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.011798\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.002596\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.170817\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.098157\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.037682\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.009251\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.190894\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.014854\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.061753\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.001677\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.002813\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.039098\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.002769\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.013849\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.007692\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.042304\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.061579\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.011985\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.057709\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.117618\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.018752\n",
            "\n",
            "Test set: Average loss: 0.0397, Accuracy: 9875/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.074479\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.011703\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.006611\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.001520\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.006622\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.003403\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.038959\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.000929\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.032976\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.057616\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.026093\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.000680\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.160151\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.030811\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.046432\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.020736\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.003128\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.060239\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.024648\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.036248\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.003581\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.008756\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.005385\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.005475\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.007402\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.023184\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.008189\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.009996\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.042437\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.002723\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.008734\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.000840\n",
            "\n",
            "Test set: Average loss: 0.0280, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "CPU times: user 14min 30s, sys: 58.9 s, total: 15min 28s\n",
            "Wall time: 15min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "69M-cw0Fk9D0"
      },
      "source": [
        "###  Federated Learning CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4D50CFvQk9D1",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "#torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "#torch.set_default_tensor_type(torch.cuda.FloatTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XwKKsXqQk9D4",
        "outputId": "65465dda-d0d5-4d3f-cb5e-e65949a3b529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import syft as sy  # <-- NEW: import the Pysyft library\n",
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0619 15:48:01.123609 140702398183296 hook.py:97] Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hnS_dMzPk9D8",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = True\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rIDMvLUNk9EA",
        "colab": {}
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z0Abt8ygk9EG",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tjcfNLSSk9EI",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        model.send(data.location) # <-- NEW: send the model to the right location\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get() # <-- NEW: get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "                100. * batch_idx / len(federated_train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ll-u9pzWk9EK",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c7c8905d-00da-41e4-da61-2cefc65a42c8",
        "id": "Nhgi58kKk9EN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "%%time\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.305134\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.156802\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.896587\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPureTorchTensorFoundError\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    198\u001b[0m             new_args, new_kwargs, new_type, args_type = syft.frameworks.torch.hook_args.hook_function_args(\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_args_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36mhook_function_args\u001b[0;34m(attr, args, kwargs, return_args_type)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Try running it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36msix_fold\u001b[0;34m(lambdas, args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m     return (\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# Last if not, rule is probably == 1 so use type to return the right transformation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0;32melse\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mforward_func\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# And do this for all the args / rules provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"child\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPureTorchTensorFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"child\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPureTorchTensorFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPureTorchTensorFoundError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8238e4a3eef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model = Net().to(device)\\noptimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\\n\\nfor epoch in range(1, args.epochs + 1):\\n    train(args, model, device, federated_train_loader, optimizer, epoch)\\n    test(args, model, device, test_loader)\\n\\nif (args.save_model):\\n    torch.save(model.state_dict(), \"mnist_cnn.pt\")'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-72ddef6ce5b6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, federated_train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-ceb0955942ca>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mcmd_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{attr.__module__}.{attr.__name__}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mnew_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;31m# Put back the wrappers where needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             response = syft.frameworks.torch.hook_args.hook_response(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/pointers/object_pointer.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Send the command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, message, return_ids)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSGTYPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, msg_type, message, location)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVirtualWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWorker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFederatedClient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseWorker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"worker {self} received {sy.codes.code2MSGTYPE[msg_type]} {contents}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Step 1: route message to appropriate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_router\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmsg_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# Step 2: Serialize the message to simple python objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;31m# some functions don't return anything (such as .backward())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mcmd_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{attr.__module__}.{attr.__name__}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;31m# in the execute_command function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 494\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m max_pool2d = torch._jit_internal.boolean_dispatch(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mcmd_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{attr.__module__}.{attr.__name__}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;31m# in the execute_command function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI3uV5L6Mv0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G0ReVW36MwPD"
      },
      "source": [
        "### SPLIT LEARNING CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w_f9LQKeMwPF",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = True\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "#device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7e176767-0363-4d6a-f4a3-bec87da3c8f0",
        "id": "5cU69GeVMwPI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 28501590.63it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 450556.00it/s]\n",
            "  1%|          | 16384/1648877 [00:00<00:11, 142488.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 7114838.39it/s]                            \n",
            "8192it [00:00, 182204.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aUH8X9LTMwPQ",
        "colab": {}
      },
      "source": [
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        return F.max_pool2d(x, 2, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X1fYDUeFMwPU",
        "colab": {}
      },
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dbKkMeGaMwPY",
        "colab": {}
      },
      "source": [
        "models = [Net1(),Net1()]\n",
        "models[0] = models[0].send(bob)\n",
        "models[1] = models[1].send(alice)\n",
        "\n",
        "\n",
        "opt1 = optim.SGD(params=models[0].parameters(),lr=0.1)\n",
        "opt2 = optim.SGD(params=models[1].parameters(),lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OIbseBcjMwPa",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targs) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        #IF ON DATA LOCATION TO GET THE RIGHT MODEL\n",
        "        if data.location.id == 'bob':\n",
        "          mod_c,opt_c = models[0], opt1\n",
        "        else : \n",
        "          mod_c,opt_c = models[1], opt2\n",
        "          \n",
        "       # 1) erase previous gradients (if they exist)\n",
        "        optimizer.step()\n",
        "        opt_c.step()\n",
        "        opt_c.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        #model.send(data.location) # <-- NEW: send the model to the right location\n",
        "        #target= tar.get()\n",
        "        tg_copy = targs.copy()\n",
        "        target = tg_copy.get()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        #optimizer.zero_grad()\n",
        "        # 2) make a prediction until cut layer (client location)\n",
        "        pred_c = mod_c(data)\n",
        "        copy = pred_c.copy()\n",
        "        # 3) get this to the server \n",
        "        inp = copy.get()\n",
        "\n",
        "        # 4) make prediction with second part of the model (server location)\n",
        "        pred = model(inp)\n",
        "\n",
        "        # 5) calculate how much we missed \n",
        "        #print(pred.size(),target.size())\n",
        "        loss = F.nll_loss(pred, target)\n",
        "        loss.backward()\n",
        "        \n",
        "        gradient = inp.grad\n",
        "        print()\n",
        "        gradient = gradient.view(inp.shape)\n",
        "        G = gradient.send(data.location)\n",
        "        print(G.shape)\n",
        "        print(pred_c.shape)\n",
        "        pred_c.backward(G)\n",
        "        print(data.grad)\n",
        "        #target = target.send(data.location)\n",
        "        #optimizer.step()\n",
        "        #model.get() # <-- NEW: get the model back\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            #loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "                100. * batch_idx / len(federated_train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lf7X3HeeMwPc",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    M1 = models[0].copy()\n",
        "    M2 = models[1].copy()\n",
        "    M1 = M1.get()\n",
        "    M2 = M2.get()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(M1(data))\n",
        "            #output2 = model(M2(data))\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ca0109c6-78ab-4b11-ed40-52fc21e37f4d",
        "id": "HaC3g2taMwPg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4510
        }
      },
      "source": [
        "%%time\n",
        "model = Net2().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)\n",
        "\n",
        "if (args.save_model):\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.312780\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.102405\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n",
            "\n",
            "torch.Size([64, 20, 12, 12])\n",
            "torch.Size([64, 20, 12, 12])\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0325af6d78a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model = Net2().to(device)\\noptimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\\n\\nfor epoch in range(1, args.epochs + 1):\\n    train(args, model, device, federated_train_loader, optimizer, epoch)\\n    test(args, model, device, test_loader)\\n\\nif (args.save_model):\\n    torch.save(model.state_dict(), \"mnist_cnn.pt\")'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-72d0044d55d1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, federated_train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfederated_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfederated_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# <-- now it is a distributed dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m#IF ON DATA LOCATION TO GET THE RIGHT MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bob'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/federated/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/federated/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/federated/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfederated_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# All the data for this worker has been used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/federated/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfederated_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# All the data for this worker has been used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/federated/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, message, return_ids)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSGTYPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, msg_type, message, location)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVirtualWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWorker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFederatedClient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseWorker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# Step 0: deserialize message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mmsg_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"worker {self} received {sy.codes.code2MSGTYPE[msg_type]} {contents}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Step 1: route message to appropriate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlOUVwEvN0AH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.tensor([[1.,1.],[1.,1.]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nqa0lKKT-dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = torch.tensor([2.,2.,2.,2.])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7WTb8yoUCZ8",
        "colab_type": "code",
        "outputId": "4de685f6-70d2-4fba-9360-ac6746442259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "b.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_opjVonUD7P",
        "colab_type": "code",
        "outputId": "98daf27d-c6c1-43c5-ec64-05de2effca1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcSZEL6qUFw-",
        "colab_type": "code",
        "outputId": "5592234b-ea6a-4755-b061-0298f17a9d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a.view(b.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW-V3bpKUJx_",
        "colab_type": "code",
        "outputId": "7199becd-1c51-44bc-bd03-196db7865501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCN_jugxUK7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}